#!/usr/bin/env python

# stdlib imports
import argparse
import logging
import os.path
import sys
import textwrap
from datetime import datetime
import warnings

# third party imports
import pandas as pd
from h5py.h5py_warnings import H5pyDeprecationWarning

# local imports
from gmprocess.args import add_shared_args
from gmprocess.io.fetch_utils import (get_events, update_config,
                                      save_shakemap_amps, download,
                                      draw_stations_map)
from gmprocess.logging import setup_logger
from gmprocess.io.asdf.stream_workspace import StreamWorkspace
from gmprocess.processing import process_streams
from gmprocess.report import build_report_latex
from gmprocess.plot import summary_plots, plot_regression
from gmprocess.config import get_config

CMD_TUPLES = [('assemble', ('Download data from all available online sources, or '
                            'load raw data from files if --directory is selected.')),
              ('process', ('Process data using steps defined '
                           'in configuration file.')),
              ('report', 'Create a summary report for each event specified.'),

              ('provenance', 'Generate provenance table in --format format.'),
              ('export', ('Generate metrics tables (NGA-style "flat" files) '
                          'for all events and IMCs.')),
              ('shakemap', ('Generate ShakeMap-friendly peak ground motions '
                            'table.'))]

TAG_FMT = '%Y%m%d%H%M%S'


class MyFormatter(argparse.RawTextHelpFormatter,
                  argparse.ArgumentDefaultsHelpFormatter):
    pass


def append_file(files_created, tag, filename):
    if tag in files_created:
        files_created[tag].append(filename)
    else:
        files_created[tag] = [filename]


def main(pparser, args):
    # get the process tag from the user or define by current datetime
    process_tag = args.process_tag or datetime.utcnow().strftime(TAG_FMT)

    # logging
    setup_logger(args)
    if args.log_file:
        logger = logging.getLogger()
        stream_handler = logger.handlers[0]
        fhandler = logging.FileHandler(args.log_file)
        logger.removeHandler(stream_handler)
        logger.addHandler(fhandler)

    # config handling
    configfile = args.config
    if configfile is not None:
        config = update_config(configfile)
        if config is None:
            print('\nCustom config file %s is invalid. Exiting.')
            sys.exit(1)

    else:
        config = get_config()

    outdir = args.outdir
    pcommands = args.commands
    all_commands = set([cmd_tuple[0] for cmd_tuple in CMD_TUPLES])
    if not set(pcommands) <= all_commands:
        invalid_commands = list(set(pcommands) - all_commands)
        print('\nThe following commands are not valid:')
        for command in invalid_commands:
            print('\t%s' % command)
        print(('\nPlease read the documentation and select from the list of '
               'appropriate pcommands.'))
        print('Exiting.\n')
        sys.exit(1)

    eventids = args.eventids
    textfile = args.textfile
    eventinfo = args.eventinfo
    input_directory = args.directory
    if not eventids and not textfile and not input_directory and not eventinfo:
        print('You must specify one of: -i, -t, -e or --directory.')
        print('Exiting.')
        sys.exit(1)

    # get a list of ScalarEvent objects from one of the inputs
    events = get_events(eventids, textfile, eventinfo, input_directory)
    if not len(events):
        print('No event information was found. Exiting.')
        sys.exit(1)

    if not os.path.isdir(outdir):
        os.makedirs(outdir)

    workspace_files = []

    files_created = {}

    for event in events:
        event_dir = os.path.join(outdir, event.id)
        if not os.path.exists(event_dir):
            os.makedirs(event_dir)

        workname = os.path.join(event_dir, 'workspace.hdf')
        workspace_exists = os.path.isfile(workname)
        workspace_has_processed = False
        workspace = None
        processing_done = False
        if workspace_exists:
            workspace_files.append(workname)
            workspace = StreamWorkspace.open(workname)
            labels = workspace.getLabels()
            labels.remove('unprocessed')
            if len(labels) == 1:
                process_tag = labels[0]
                workspace_has_processed = True
            else:
                if 'process' not in pcommands:
                    fmt = '\nThere are %i sets of processed data in %s.'
                    tpl = (len(labels), workname)
                    print(fmt % tpl)
                    print(('This software can only handle one set of '
                           'processed data. Exiting.\n'))
                    sys.exit(1)

        download_done = False
        if 'assemble' in pcommands:
            logging.info('Downloading/loading raw streams...')
            workspace, workspace_file, rstreams = download(event, event_dir,
                                                           config,
                                                           input_directory)

            download_done = True
            append_file(files_created, 'Workspace', workname)
        else:
            if not workspace_exists:
                print('\nYou opted not to download or process from input.')
                print('No previous HDF workspace file could be found.')
                print('Try re-running with the assemble command with or ')
                print('without the --directory option.\n')
                sys.exit(1)
            if 'process' in pcommands:
                logging.info('Getting raw streams from workspace...')
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore",
                                          category=H5pyDeprecationWarning)
                    rstreams = workspace.getStreams(event.id,
                                                    labels=['unprocessed'])
                download_done = True
            else:
                need_processed = set(['report', 'shakemap'])
                need_pstreams = len(need_processed.intersection(pcommands))
                if workspace_has_processed:
                    if need_pstreams:
                        logging.info(
                            'Getting processed streams from workspace...')
                        with warnings.catch_warnings():
                            warnings.simplefilter("ignore",
                                                  category=H5pyDeprecationWarning)
                            pstreams = workspace.getStreams(event.id,
                                                            labels=[process_tag])
                    download_done = True
                    processing_done = True

        if workname not in workspace_files:
            workspace_files.append(workname)

        if 'process' in pcommands and download_done and not processing_done:
            logging.info('Processing raw streams for event %s...' % event.id)
            pstreams = process_streams(rstreams, event, config=config)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore",
                                      category=H5pyDeprecationWarning)
                workspace.addStreams(event, pstreams, label=process_tag)
            processing_done = True

        reporting_done = False
        if 'report' in pcommands and processing_done:
            logging.info(
                'Creating diagnostic plots for event %s...' % event.id)
            plot_dir = os.path.join(event_dir, 'plots')
            if not os.path.isdir(plot_dir):
                os.makedirs(plot_dir)
            for stream in pstreams:
                summary_plots(stream, plot_dir, event)

            mapfile = draw_stations_map(pstreams, event, event_dir)
            append_file(files_created, 'Station map', mapfile)

            logging.info(
                'Creating diagnostic report for event %s...' % event.id)
            # Build the summary report?
            build_conf = config['build_report']
            report_format = build_conf['format']
            if report_format == 'latex':
                report_file, success = build_report_latex(pstreams,
                                                          event_dir,
                                                          plot_dir,
                                                          event, config=config)
            else:
                report_file = ''
                success = False
            if os.path.isfile(report_file) and success:
                reporting_done = True
                append_file(files_created, 'Summary report', report_file)

        provenance_done = False
        if 'provenance' in pcommands and processing_done:
            logging.info(
                'Creating provenance table for event %s...' % event.id)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore",
                                      category=H5pyDeprecationWarning)
                provdata = workspace.getProvenance(event.id,
                                                   labels=[process_tag])
            if args.format == 'csv':
                csvfile = os.path.join(event_dir, 'provenance.csv')
                append_file(files_created, 'Provenance', csvfile)
                provdata.to_csv(csvfile)
            else:
                excelfile = os.path.join(event_dir, 'provenance.xlsx')
                append_file(files_created, 'Provenance', excelfile)
                provdata.to_excel(excelfile, index=False)
            provenance_done = True

        shakemap_done = False
        if 'shakemap' in pcommands and processing_done:
            logging.info(
                'Creating shakemap table for event %s...' % event.id)
            shakemap_file = save_shakemap_amps(pstreams, event, event_dir)
            shakemap_done = True
            append_file(files_created, 'shakemap', shakemap_file)

        workspace.close()

    # if n_passed is zero, don't
    if 'export' in pcommands and processing_done and len(workspace_files):
        event_table = None
        imc_tables = {}
        for workspace_file in workspace_files:
            workspace = StreamWorkspace.open(workspace_file)
            labels = workspace.getLabels()
            labels.remove('unprocessed')
            if len(labels) == 0:
                fmt = ('Workspace file "%s" appears to have no processed data. '
                       'Skipping.')
                print(fmt % workspace_file)
                continue
            eventid = workspace.getEventIds()[0]
            logging.info(
                'Creating flatfile tables for event %s...' % eventid)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore",
                                      category=H5pyDeprecationWarning)
                tevent_table, timc_tables = workspace.getFlatTables(
                    labels[0])
            if event_table is None:
                event_table = tevent_table
            else:
                event_table = pd.concat([event_table, tevent_table])
            if not len(imc_tables):
                imc_tables = timc_tables
            else:
                for imc, imc_table in imc_tables.items():
                    if imc in timc_tables:
                        timc_table = timc_tables[imc]
                        imc_tables[imc] = pd.concat([imc_table, timc_table])
            workspace.close()

        if args.format == 'csv':
            eventfile = os.path.join(outdir, 'events.csv')
            event_table.to_csv(eventfile, index=False)
            append_file(files_created, 'Event table', eventfile)
            for imc, imc_table in imc_tables.items():
                imcfile = os.path.join(outdir, '%s.csv' % imc.lower())
                imc_table.to_csv(imcfile, index=False)
                append_file(files_created, 'IMC tables', imcfile)
        else:
            eventfile = os.path.join(outdir, 'events.xlsx')
            event_table.to_excel(eventfile, index=False)
            append_file(files_created, 'Event table', eventfile)
            for imc, imc_table in imc_tables.items():
                imcfile = os.path.join(outdir, '%s.xlsx' % imc.lower())
                imc_table.to_excel(imcfile, index=False)
                append_file(files_created, 'IMC tables', imcfile)

        # make a regression plot of the most common imc/imt combination we can find
        pref_imcs = ['GREATER_OF_TWO_HORIZONTALS', 'ROTD50']
        pref_imts = ['PGA', 'PGV', 'SA(1.0)']
        found_imc = None
        found_imt = None
        for imc in pref_imcs:
            if imc in imc_tables:
                for imt in pref_imts:
                    if imt in imc_tables[imc].columns:
                        found_imt = imt
                        found_imc = imc
                        break
                if found_imc:
                    break
        if found_imc and found_imt:
            pngfile = '%s_%s.png' % (found_imc, found_imt)
            regression_file = os.path.join(outdir, pngfile)
            plot_regression(event_table, imc_tables[found_imc],
                            found_imt, regression_file,
                            distance_metric='EpicentralDistance',
                            colormap='jet')
            append_file(files_created,
                        'Multi-event regression plot', regression_file)

    print('\nThe following files have been created:')
    for file_type, file_list in files_created.items():
        print('File type: %s' % file_type)
        for fname in file_list:
            print('\t%s' % fname)
    print('\nProcessing is complete.\n')


def get_command_help():
    nuggets = []
    for command, description in CMD_TUPLES:
        nugget = ' -%s: %s' % (command, description)
        nuggets.append(nugget)

    command_string = '\n' + '\n'.join(nuggets)
    return command_string


if __name__ == '__main__':
    description = '''Download, process, and extract metrics from raw ground motion data.

    This program will allow the user to:
     - download raw data from a number of sources, including:
       - Any FDSN provider which serves waveform data
       - Japan's KNET/KikNet repository (requires login info)
       - 
    '''

    parser = argparse.ArgumentParser(
        description=description, formatter_class=MyFormatter)

    # ***************positional arguments
    parser.add_argument('outdir', help='Output file directory', type=str)

    # command arguments
    cmd_help = get_command_help()
    parser.add_argument('commands', help=cmd_help, type=str, nargs='+')

    # ***************optional arguments
    parser.add_argument('-i', '--eventids',
                        help='ComCat Event IDs', nargs='+')

    text_help = '''Text file containing lines of ComCat Event IDs or
    event information (ID TIME LAT LON DEPTH MAG)
    '''
    parser.add_argument('-t', '--textfile',
                        help=text_help)

    event_help = ('Single event information as ID TIME(YYYY-MM-DDTHH:MM:SS) '
                  'LAT LON DEP MAG')
    parser.add_argument('-e', '--eventinfo', type=str, nargs=6,
                        metavar=('ID', 'TIME', 'LAT', 'LON', 'DEPTH', 'MAG'),
                        help=event_help)

    hstr = ('Sidestep online data retrieval, read from local directory. '
            'This directory should contain any number of event data '
            'directories, which should contain data files in a known format '
            'and an event.json file, which should be the JSON form of a '
            'dictionary with fields: id, time, lat, lon, depth, magnitude. '
            'The id field must match the event directory name.'
            )
    hstr = '\n'.join(textwrap.wrap(hstr))
    parser.add_argument('--directory', help=hstr)

    parser.add_argument('-f', '--format',
                        help='Output format for tabular information',
                        choices=['excel', 'csv'], default='csv')

    tag_help = ('Processing label (single word, no spaces) to attach to processed files. '
                'Defaults to the current time in YYYYMMDDHHMMSS format.')
    parser.add_argument('-p', '--process-tag',
                        help=tag_help)

    parser.add_argument('-c', '--config',
                        help='Supply custom configuration file')

    parser.add_argument('-l', '--log-file',
                        help='Supply file name to store processing log info.')

    # Shared arguments
    parser = add_shared_args(parser)

    pargs = parser.parse_args()
    main(parser, pargs)
