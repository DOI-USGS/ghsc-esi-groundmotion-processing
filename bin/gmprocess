#!/usr/bin/env python

# stdlib imports
import argparse
import logging
import os.path
import sys
import textwrap
from datetime import datetime
import warnings

# third party imports
import pandas as pd
from h5py.h5py_warnings import H5pyDeprecationWarning
import psutil

# local imports
from gmprocess.args import add_shared_args
from gmprocess.io.fetch_utils import (get_events, update_config,
                                      save_shakemap_amps, download,
                                      draw_stations_map)
from gmprocess.logging import setup_logger
from gmprocess.io.asdf.stream_workspace import StreamWorkspace
from gmprocess.processing import process_streams
from gmprocess.report import build_report_latex
from gmprocess.plot import summary_plots, plot_regression
from gmprocess.config import get_config

NON_IMT_COLS = set(['EarthquakeId',
                    'Network',
                    'NetworkDescription',
                    'StationCode',
                    'StationID',
                    'StationDescription',
                    'StationLatitude',
                    'StationLongitude',
                    'StationElevation',
                    'SamplingRate',
                    'EpicentralDistance',
                    'HypocentralDistance',
                    'H1Lowpass',
                    'H1Highpass',
                    'H2Lowpass',
                    'H2Highpass',
                    'SourceFile'])

TAG_FMT = '%Y%m%d%H%M%S'


class MyFormatter(argparse.RawTextHelpFormatter,
                  argparse.ArgumentDefaultsHelpFormatter):
    pass


def format_helptext(text):
    '''Format help text, including wrapping.
    '''
    return '\n'.join(textwrap.wrap(text))


def append_file(files_created, tag, filename):
    if tag in files_created:
        files_created[tag].append(filename)
    else:
        files_created[tag] = [filename]


def main(args):
    # get the process tag from the user or define by current datetime
    process_tag = args.process_tag or datetime.utcnow().strftime(TAG_FMT)

    # logging
    setup_logger(args)
    if args.log_file:
        logger = logging.getLogger()
        stream_handler = logger.handlers[0]
        fhandler = logging.FileHandler(args.log_file)
        logger.removeHandler(stream_handler)
        logger.addHandler(fhandler)

    # config handling
    configfile = args.config
    if configfile is not None:
        config = update_config(configfile)
        if config is None:
            print('\nCustom config file %s is invalid. Exiting.')
            sys.exit(1)

    else:
        config = get_config()

    outdir = args.outdir

    eventids = args.eventids
    textfile = args.textfile
    eventinfo = args.eventinfo
    input_directory = args.directory

    # get a list of ScalarEvent objects from one of the inputs
    events = get_events(eventids, textfile, eventinfo, input_directory)
    if not events:
        print('No event information was found. Exiting.')
        sys.exit(1)

    if not os.path.isdir(outdir):
        os.makedirs(outdir)

    workspace_files = []
    files_created = {}

    for event in events:
        event_dir = os.path.join(outdir, event.id)
        if not os.path.exists(event_dir):
            os.makedirs(event_dir)

        workname = os.path.join(event_dir, 'workspace.hdf')
        workspace_exists = os.path.isfile(workname)
        workspace_has_processed = False
        workspace = None
        rstreams = None
        pstreams = None
        processing_done = False
        if workspace_exists:
            workspace_files.append(workname)
            workspace = StreamWorkspace.open(workname)
            labels = workspace.getLabels()
            if labels:
                labels.remove('unprocessed')
            if len(labels) == 1:
                process_tag = labels[0]
                workspace_has_processed = True
            else:
                if not args.process:
                    print(format_helptext(
                        '\nThere are {} sets of processed data in {}.'
                        'This software can only handle one set of '
                        'processed data. Exiting.'.format(
                            len(labels), workname)))
                    sys.exit(1)

        download_done = False
        if args.assemble:
            logging.info('Downloading/loading raw streams...')
            workspace, workspace_file, rstreams = download(event, event_dir,
                                                           config,
                                                           input_directory)

            download_done = True
            append_file(files_created, 'Workspace', workname)
        else:
            if not workspace_exists:
                print(format_helptext(
                    '\n'
                    'You opted not to download or process from input. No '
                    'previous HDF workspace file could be found. Try re-running '
                    'with the assemble command with or without the --directory '
                    'option.'
                    ))
                sys.exit(1)
            if args.process:
                logging.info('Getting raw streams from workspace...')
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore",
                                          category=H5pyDeprecationWarning)
                    rstreams = workspace.getStreams(event.id,
                                                    labels=['unprocessed'])
                download_done = True
            else:
                need_pstreams = args.report or args.shakemap
                if workspace_has_processed:
                    if need_pstreams:
                        logging.info(
                            'Getting processed streams from workspace...')
                        with warnings.catch_warnings():
                            warnings.simplefilter(
                                "ignore", category=H5pyDeprecationWarning)
                            pstreams = workspace.getStreams(
                                event.id, labels=[process_tag])
                    download_done = True
                    processing_done = True

        if workname not in workspace_files:
            workspace_files.append(workname)

        if args.process and download_done and not processing_done:
            logging.info('Processing raw streams for event %s...', event.id)
            pstreams = process_streams(rstreams, event, config=config)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore",
                                      category=H5pyDeprecationWarning)
                workspace.addStreams(event, pstreams, label=process_tag)
                workspace.calcMetrics(event.id,
                                      labels=[process_tag],
                                      config=config)
            processing_done = True

        reporting_done = False
        if args.report and processing_done:
            logging.info(
                'Creating diagnostic plots for event %s...', event.id)
            plot_dir = os.path.join(event_dir, 'plots')
            if not os.path.isdir(plot_dir):
                os.makedirs(plot_dir)
            for stream in pstreams:
                summary_plots(stream, plot_dir, event)

            mapfile = draw_stations_map(pstreams, event, event_dir)
            append_file(files_created, 'Station map', mapfile)

            logging.info(
                'Creating diagnostic report for event %s...', event.id)
            # Build the summary report?
            build_conf = config['build_report']
            report_format = build_conf['format']
            if report_format == 'latex':
                report_file, success = build_report_latex(
                    pstreams,
                    event_dir,
                    event,
                    config=config
                )
            else:
                report_file = ''
                success = False
            if os.path.isfile(report_file) and success:
                reporting_done = True
                append_file(files_created, 'Summary report', report_file)

        provenance_done = False
        if args.provenance and processing_done:
            logging.info(
                'Creating provenance table for event %s...', event.id)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore",
                                      category=H5pyDeprecationWarning)
                provdata = workspace.getProvenance(event.id,
                                                   labels=[process_tag])
            if args.format == 'csv':
                csvfile = os.path.join(event_dir, 'provenance.csv')
                append_file(files_created, 'Provenance', csvfile)
                provdata.to_csv(csvfile)
            else:
                excelfile = os.path.join(event_dir, 'provenance.xlsx')
                append_file(files_created, 'Provenance', excelfile)
                provdata.to_excel(excelfile, index=False)
            provenance_done = True

        shakemap_done = False
        if args.shakemap and processing_done:
            logging.info(
                'Creating shakemap table for event %s...', event.id)
            shakemap_file = save_shakemap_amps(pstreams, event, event_dir)
            shakemap_done = True
            append_file(files_created, 'shakemap', shakemap_file)

        workspace.close()
        # since we don't know how many events users will be processing,
        # let's guard against memory issues by clearing out the big data
        # structures
        # del rstreams
        # del pstreams
        pmem = psutil.virtual_memory()
        pct = pmem.used / pmem.total
        fmt = 'Finishing event %s (%i streams) - memory usage = %.2f %%'
        tpl = (event.id, len(rstreams), pct)
        logging.info(fmt % tpl)

    # if n_passed is zero, don't
    if args.export and processing_done and workspace_files:
        event_table = None
        imc_tables = {}
        for workspace_file in workspace_files:
            workspace = StreamWorkspace.open(workspace_file)
            labels = workspace.getLabels()
            labels.remove('unprocessed')
            if not labels:
                fmt = ('Workspace file "%s" appears to have no processed '
                       'data. Skipping.')
                print(fmt % workspace_file)
                continue
            eventid = workspace.getEventIds()[0]
            logging.info(
                'Creating tables for event %s...', eventid)
            with warnings.catch_warnings():
                warnings.simplefilter("ignore",
                                      category=H5pyDeprecationWarning)
                if args.recompute_metrics:
                    tevent_table, timc_tables = workspace.getTables(
                        labels[0], config=config)
                else:
                    tevent_table, timc_tables = workspace.getTables(
                        labels[0], config=None)
            if event_table is None:
                event_table = tevent_table
            else:
                event_table = pd.concat([event_table, tevent_table])
            if not imc_tables:
                imc_tables = timc_tables
            else:
                for imc, imc_table in imc_tables.items():
                    if imc in timc_tables:
                        timc_table = timc_tables[imc]
                        imc_tables[imc] = pd.concat([imc_table, timc_table])
            workspace.close()

        if args.format == 'csv':
            eventfile = os.path.join(outdir, 'events.csv')
            event_table.to_csv(eventfile, index=False)
            append_file(files_created, 'Event table', eventfile)
            for imc, imc_table in imc_tables.items():
                imcfile = os.path.join(outdir, '%s.csv' % imc.lower())
                imc_table.to_csv(imcfile, index=False)
                append_file(files_created, 'IMC tables', imcfile)
        else:
            eventfile = os.path.join(outdir, 'events.xlsx')
            event_table.to_excel(eventfile, index=False)
            append_file(files_created, 'Event table', eventfile)
            for imc, imc_table in imc_tables.items():
                imcfile = os.path.join(outdir, '%s.xlsx' % imc.lower())
                imc_table.to_excel(imcfile, index=False)
                append_file(files_created, 'IMC tables', imcfile)

        # make a regression plot of the most common imc/imt combination we
        # can find
        pref_imcs = ['GREATER_OF_TWO_HORIZONTALS', 'H1', 'H2']
        pref_imts = ['PGA', 'PGV', 'SA(1.0)']
        found_imc = None
        found_imt = None
        for imc in pref_imcs:
            if imc in imc_tables:
                for imt in pref_imts:
                    if imt in imc_tables[imc].columns:
                        found_imt = imt
                        found_imc = imc
                        break
                if found_imc:
                    break
        # now look for whatever IMC/IMTcombination we can find
        if imc_tables and not found_imc:
            found_imc = list(imc_tables.keys())[0]
            table_cols = set(imc_tables[found_imc].columns)
            imtlist = list(table_cols - NON_IMT_COLS)
            found_imt = imtlist[0]

        if found_imc and found_imt:
            pngfile = '%s_%s.png' % (found_imc, found_imt)
            regression_file = os.path.join(outdir, pngfile)
            plot_regression(event_table, found_imc,
                            imc_tables[found_imc],
                            found_imt,
                            regression_file,
                            distance_metric='EpicentralDistance',
                            colormap='viridis_r')
            append_file(files_created,
                        'Multi-event regression plot', regression_file)

    print('\nThe following files have been created:')
    for file_type, file_list in files_created.items():
        print('File type: %s' % file_type)
        for fname in file_list:
            print('\t%s' % fname)
    print('\nProcessing is complete.\n')


if __name__ == '__main__':
    description = '''Download, process, and extract metrics from raw ground motion data.

This program will allow the user to:
   - download raw data from a number of sources, including:
   - Any FDSN provider which serves waveform data
   - Japan's KNET/KikNet repository (requires login info)
   - ...
'''

    parser = argparse.ArgumentParser(
        description=description, formatter_class=MyFormatter)

    # ***** Required arguments
    parser.add_argument('-o', '--output-directory', help='Output directory',
                        metavar="DIRECTORY", action='store', type=str,
                        required=True, dest='outdir')

    # ***** Command arguments
    help_assemble = format_helptext(
        'Download data from all available online sources, or load raw data from '
        'files if --directory is selected.'
        )
    parser.add_argument('--assemble', help=help_assemble,
                        action='store_true', dest='assemble')

    help_process = format_helptext(
        'Process data using steps defined in configuration file.'
        )
    parser.add_argument('--process', help=help_process,
                        action='store_true', dest='process')

    help_report = format_helptext(
        'Create a summary report for each event specified.'
        )
    parser.add_argument('--report', help=help_report, action='store_true',
                        dest='report')

    help_provenance = format_helptext(
        'Generate provenance table in --format format.'
        )
    parser.add_argument('--provenance', help=help_provenance,
                        action='store_true', dest='provenance')

    help_export = format_helptext(
        'Generate metrics tables (NGA-style "flat" files) for all events '
        'and IMCs.'
        )
    parser.add_argument('--export', help=help_export, action='store_true',
                        dest='export')

    help_shakemap = format_helptext(
        'Generate ShakeMap-friendly peak ground motions table.'
        )
    parser.add_argument('--shakemap', help=help_shakemap,
                        action='store_true', dest='shakemap')

    # # ***** Optional arguments
    group = parser.add_mutually_exclusive_group(required=True)
    help_eventids = format_helptext(
        'ComCat Event IDs'
        )
    group.add_argument('--eventids', help=help_eventids, nargs='+')

    help_textfile = format_helptext(
        'Text file containing lines of ComCat Event IDs or event '
        'information (ID TIME LAT LON DEPTH MAG)'
        )
    group.add_argument('--textfile', help=help_textfile, action='store',
                       dest='textfile')

    help_event = format_helptext(
        'Single event information as ID TIME(YYYY-MM-DDTHH:MM:SS) LAT LON DEP MAG'
        )
    group.add_argument('--eventinfo', help=help_event, type=str, nargs=6,
                       metavar=('ID', 'TIME', 'LAT', 'LON', 'DEPTH', 'MAG'))

    help_dir = format_helptext(
        'Sidestep online data retrieval, read from local directory. This '
        'directory should contain any number of event data directories, which '
        'should contain data files in a known format and an event.json file, '
        'which should be the JSON form of a dictionary with fields: id, time, '
        'lat, lon, depth, magnitude. The id field must match the event '
        'directory name.'
        )
    group.add_argument('--directory', help=help_dir, action='store',
                       dest='directory')

    help_format = format_helptext(
        'Output format for tabular information'
        )
    parser.add_argument('--format', help=help_format,
                        choices=['excel', 'csv'], default='csv', dest='format')

    help_tag = format_helptext(
        'Processing label (single word, no spaces) to attach to processed files.'
        'Defaults to the current time in YYYYMMDDHHMMSS format.'
        )
    parser.add_argument('--process-tag', help=help_tag, action='store',
                        type=str, dest='process_tag')

    help_config = format_helptext(
        'Supply custom configuration file'
        )
    parser.add_argument('--config', help=help_config, action='store',
                        type=str, dest='config')

    help_recompute = format_helptext(
        'Recompute metrics (i.e. from new config)'
        )
    parser.add_argument('--recompute-metrics', help=help_recompute,
                        action='store_true', dest='recompute_metrics')

    help_logfile = format_helptext(
        'Supply file name to store processing log info.'
        )
    parser.add_argument('--log-file', help=help_logfile, action='store',
                        dest='log_file')

    # ***** Shared arguments
    parser = add_shared_args(parser)
    pargs = parser.parse_args()
    main(pargs)
